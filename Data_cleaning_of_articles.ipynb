{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fghQa79gKziW"},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import re\n","import os"]},{"cell_type":"code","source":["# Download required nltk packages\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"metadata":{"id":"CBAd65F3LSA6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eebedd6d-7a35-4cc9-c964-dfcc7a729c60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Load data (replace 'data.csv' with your actual file)\n","# Assuming data has a column 'text' with the news content\n","df = pd.read_csv('/content/scraped_article 1.csv')"],"metadata":{"id":"zNX9gVQgMz9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to clean text\n","def clean_text(text):\n","    # Check if the input is a string\n","    if isinstance(text, str):\n","        # Convert to lowercase\n","        text = text.lower()\n","\n","        # Remove URLs\n","        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","\n","        # Remove punctuation and numbers\n","        text = re.sub(r'\\d+', '', text)\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","\n","        # Remove special characters\n","        text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n","\n","        # Tokenize\n","        tokens = word_tokenize(text)\n","\n","        # Remove stopwords\n","        stop_words = set(stopwords.words('english'))\n","        tokens = [word for word in tokens if word not in stop_words]\n","\n","        # Lemmatize\n","        lemmatizer = WordNetLemmatizer()\n","        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","        # Join tokens back into string\n","        cleaned_text = ' '.join(tokens)\n","\n","        return cleaned_text\n","    else:\n","        # Handle non-string values (e.g., float)\n","        # You can return an empty string, a placeholder, or handle it differently\n","        return ''  # Return an empty string for non-string values"],"metadata":{"id":"uF5zXyHyLXoH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Directory with CSV files\n","input_directory = '/content/articles'\n","output_directory = '/content/cleaned articles'\n"],"metadata":{"id":"lRmxk36Zofy4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ensure output directory exists\n","os.makedirs(output_directory, exist_ok=True)"],"metadata":{"id":"EuI6K6U1ojIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for filename in os.listdir(input_directory):\n","    if filename.endswith('.csv'):\n","        # Load data\n","        filepath = os.path.join(input_directory, filename)\n","        df = pd.read_csv(filepath)\n","\n","        # Clean text column (assuming 'text' is the column name with news content)\n","        if 'Paragraph' in df.columns:\n","            df['cleaned_text'] = df['Paragraph'].apply(clean_text)\n","\n","            # Save cleaned data to a new CSV file in the output directory\n","            output_filepath = os.path.join(output_directory, f'cleaned_{filename}')\n","            df.to_csv(output_filepath, index=False)\n","            print(f\"Processed and saved: {output_filepath}\")\n","        else:\n","            print(f\"Column 'Paragraph' not found in {filename}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ApLT1L3qpWNY","outputId":"1ed9053a-fa7e-4ca4-b10a-a6bbe27f68a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed and saved: /content/cleaned articles/cleaned_scraped_article 2.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 13.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 20.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 17.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 15.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 11.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 1.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 19.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 8.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 3.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 4.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 7.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 10.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 9.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 16.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 6.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 14.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 12.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 5.csv\n","Processed and saved: /content/cleaned articles/cleaned_scraped_article 18.csv\n"]}]},{"cell_type":"code","source":["# Apply cleaning function to the dataset\n","# Assuming your column is named 'Paragraph'\n","df['cleaned_text'] = df['Paragraph'].apply(clean_text)\n","\n","# Preview cleaned data\n","print(df[['Paragraph', 'cleaned_text']].head()) # Changed 'text' to 'Paragraph'\n","\n","# Save cleaned data (optional)\n","df.to_csv('cleaned_data.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SxKpfwlL3L7","outputId":"b5e12741-5405-4eeb-a348-835076acaf37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                           Paragraph  \\\n","0                       To enjoy additional benefits   \n","1                                    CONNECT WITH US   \n","2  \\n\\t\\t\\t\\t\\tCopyright© 2024, THG PUBLISHING PV...   \n","3                                        BACK TO TOP   \n","4    Terms & conditions  |  Institutional Subscriber   \n","\n","                                        cleaned_text  \n","0                           enjoy additional benefit  \n","1                                          connect u  \n","2  copyright thg publishing pvt ltd affiliated co...  \n","3                                           back top  \n","4            term condition institutional subscriber  \n"]}]},{"cell_type":"code","source":["# Directory containing the cleaned CSV files\n","input_directory = '/content/cleaned articles'\n","output_file = 'combined_cleaned_data.csv'"],"metadata":{"id":"KO0zgMY6Tk55"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize an empty list to hold the data\n","data_frames = []\n"],"metadata":{"id":"aAbr--RnrKjU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Iterate over all cleaned CSV files in the directory\n","for filename in os.listdir(input_directory):\n","    if filename.startswith('cleaned_') and filename.endswith('.csv'):\n","        filepath = os.path.join(input_directory, filename)\n","\n","        # Read each cleaned CSV file and append it to the list\n","        df = pd.read_csv(filepath)\n","        data_frames.append(df)\n"],"metadata":{"id":"PVqPCE55rMgn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Concatenate all dataframes in the list\n","combined_df = pd.concat(data_frames, ignore_index=True)"],"metadata":{"id":"MxSheEfzrPrD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the combined data to a single CSV file\n","combined_df.to_csv(output_file, index=False)\n","print(f\"All cleaned files have been combined and saved as {output_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qi02wf_FrSX5","outputId":"4d2a4f04-ced6-45b2-d962-5f474298409d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All cleaned files have been combined and saved as combined_cleaned_data.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"JRnrMGhNrdNs"},"execution_count":null,"outputs":[]}]}